{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CompiladoTwitSentiment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1EKJc75QxBXaxhretS6jS9nvr0E-JBfL8",
      "authorship_tag": "ABX9TyMwBp56mGAczlyJk3WGTu34",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IkeSalmonson/Projetos/blob/master/CompiladoTwitSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZkYJF7e4mCy"
      },
      "source": [
        "# Capitura Twit \r\n",
        "  & salva no drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8HhGoji4Z_G",
        "outputId": "3e549937-054a-444c-bf52-0a27af728479"
      },
      "source": [
        "#Imports e classes \n",
        "import os\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "import time\n",
        "\n",
        "import tweepy\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Creating a StreamListener class to catch the stream from  tweepy.Stream\n",
        "class MyStreamListener(tweepy.StreamListener):\n",
        "  tweetsCounter = 0 #conta quantos tweets foram lidos\n",
        "  tweetsWanted = 2\n",
        "  tweets = {}\n",
        "  def on_status(self,status):\n",
        "\n",
        "    \n",
        "    self.tweets[self.tweetsCounter] = status._json \n",
        "    self.tweetsCounter +=1\n",
        "\n",
        "    if self.tweetsWanted < 10:\n",
        "      print(str(self.tweetsCounter) +\" \" +status)\n",
        "    else:\n",
        "      print(str(self.tweetsCounter) +\" \")  \n",
        "    if self.tweetsCounter < self.tweetsWanted :\n",
        "      return True\n",
        "    else :\n",
        "      return False  \n",
        "\n",
        "#provavelmente deveria estar usando o on_data e não on_status\n",
        "  def on_error(self,status_code):\n",
        "    if status_code == 420:\n",
        "      return False\n",
        "      #returning false disconects the stream\n",
        "       # returning non-False reconnects the stream, with backoff. \n",
        "\n",
        "\n",
        "#Salvar em formato JSON no drive com timestamp no nome\n",
        "\n",
        "class SaveToDrive(MyStreamListener):\n",
        "  \n",
        "\n",
        "  savePath = 'drive/My Drive/TwitterStream/'\n",
        "  \n",
        "  def save (self):\n",
        "    t = time.localtime()\n",
        "    current_time = time.strftime(\"%d_%m_%H_%M\", t)\n",
        "    fileName = current_time+'_tweets.json'\n",
        "    with open( self.savePath + fileName, 'w') as fileName :\n",
        "     json.dump(myStreamListener.tweets, fileName, indent = 4)\n",
        "\n",
        "\n",
        "print('done'+time.strftime(\"%d %b %Y %H:%M:%S\", time.localtime()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "done14 Dec 2020 13:48:30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhviLNtR5Bi2",
        "outputId": "d0380456-0d97-4395-e9e6-01694285b3f8"
      },
      "source": [
        "#rodar para preparar autorização da API\n",
        "\n",
        "# Transformar em método do MyStreamListener\n",
        "\n",
        "authPath = 'drive/My Drive/TwitterStream/Auth/'\n",
        "with open (authPath+'auth.json', 'r',) as file:\n",
        "  auth = json.load(file)\n",
        "\n",
        "consumer_key = ''\n",
        "consumer_secret = ''\n",
        "access_token = ''\n",
        "access_token_secret = ''\n",
        "\n",
        "for key in auth['auth']:\n",
        "  consumer_key = key['consumer_key']\n",
        "  consumer_secret =key['consumer_secret'] \n",
        "  access_token = key ['access_token']\n",
        "  access_token_secret =key ['access_token_secret']\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "print('done'+time.strftime(\"%d %b %Y %H:%M:%S\", time.localtime()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done14 Dec 2020 13:51:04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLC4273CC3F0",
        "outputId": "7c2e56a0-826a-4ae0-ea12-4b9ebe8aaa98"
      },
      "source": [
        "#Rodar para capiturar os tweets\n",
        "\n",
        "#Aparentement ao rodar novamente não redefine filtro e conteudo dos objetos\n",
        "\n",
        "#Definir as listas para filtro. Os filtros são aditivos ( OR ) qualquer umas das \n",
        "filterContent= ['mask'] \n",
        "filterLocation = []\n",
        "filterLanguage = ['en'] # linguas de interesse : en (English) pt (Portuguese)  de (German)  fr (French) ja (Japanese)\n",
        "\n",
        "\n",
        "tweetsWanted = 10\n",
        "\n",
        "# Creating a Stream\n",
        "myStreamListener = MyStreamListener()\n",
        "myStreamListener.tweetsWanted = tweetsWanted\n",
        "\n",
        "myStream = tweepy.Stream(auth = api.auth, listener = myStreamListener)\n",
        "myStream.filter(track=filterContent,\n",
        "                #locations =filterLocation,\n",
        "                languages = filterLanguage      \n",
        "                )\n",
        "\n",
        "#salva a lista tweets no arquivo\n",
        "saveToDrive = SaveToDrive(myStreamListener)\n",
        "saveToDrive.save()\n",
        "\n",
        "myStream.disconnect()\n",
        "print('done '+time.strftime(\"%d %b %Y %H:%M:%S\", time.localtime()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 \n",
            "2 \n",
            "3 \n",
            "4 \n",
            "5 \n",
            "6 \n",
            "7 \n",
            "8 \n",
            "9 \n",
            "10 \n",
            "done 14 Dec 2020 13:51:10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ol_ZRVY4iPT"
      },
      "source": [
        "# Pretratamento e Classificação\r\n",
        "com NLTK e SKlearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuVmaxF-4hvB",
        "outputId": "28dbb884-e24c-4582-cca6-756b8f5394b4"
      },
      "source": [
        "#exatamente igual ao video: + donwloads necessarios, regex e time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import random\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import pickle\n",
        "#Apenas os classificadores que vou usar\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import LinearSVC , NuSVC\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "from nltk.tokenize import word_tokenize\n",
        "from google.colab import drive #Se não montar o drive automaticamente na pasta clicar no botão em \"files/mountDrive\"\n",
        "import time\n",
        "import re\n",
        "print('done'+time.strftime(\"%d %b %Y %H:%M:%S\", time.localtime()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "done14 Dec 2020 13:51:15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v1q5tPUDhCa",
        "outputId": "2686ef11-4bcd-4fa0-ce7c-e7e8807865d6"
      },
      "source": [
        "#load short pickled classifiers and test accuracy \n",
        "\n",
        "object_list = [\"short_SGDClassifier_classifier\", \"short_LinearSVC_classifier\",\n",
        " \"short_NuSVC_classifier\"\n",
        "]\n",
        "object_short_pickle_load = []\n",
        "\n",
        "for object_2load in object_list:\n",
        "  path = '/content/drive/My Drive/TwitterStream/Pickle/'+object_2load+'.pickle' \n",
        "  print(path)\n",
        "  load_file = open( path , \"rb\") #rb = write binary\n",
        "  object_short_pickle_load.append(pickle.load(load_file))\n",
        "  load_file.close()\n",
        "\n",
        "print(object_short_pickle_load) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TwitterStream/Pickle/short_SGDClassifier_classifier.pickle\n",
            "/content/drive/My Drive/TwitterStream/Pickle/short_LinearSVC_classifier.pickle\n",
            "/content/drive/My Drive/TwitterStream/Pickle/short_NuSVC_classifier.pickle\n",
            "[<SklearnClassifier(SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False))>, <SklearnClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0))>, <SklearnClassifier(NuSVC(break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "      decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "      max_iter=-1, nu=0.5, probability=False, random_state=None, shrinking=True,\n",
            "      tol=0.001, verbose=False))>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmC_bRBEFjuA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Md-GmyhF5up",
        "outputId": "230d344b-53e8-4c6d-ee1b-df6e61ce389c"
      },
      "source": [
        "def find_short_features(review):\n",
        "  words= nltk.word_tokenize(review)\n",
        "  features = {}\n",
        "  for w in words:\n",
        "    features[w] = (w in words)\n",
        "  return features \n",
        "\n",
        "# short_features = find_short_features(short_documents)\n",
        "\n",
        "testString = [(\"I liked it\",\"pos\"),(\"It was bad\", \"neg\"),(\"I loved it\",\"pos\"),(\"It had bad acting and terrible photography\", \"neg\")]\n",
        "short_features_set = [(find_short_features(rev),category) for (rev, category) in testString ]\n",
        "\n",
        "print(short_features_set) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[({'I': True, 'liked': True, 'it': True}, 'pos'), ({'It': True, 'was': True, 'bad': True}, 'neg'), ({'I': True, 'loved': True, 'it': True}, 'pos'), ({'It': True, 'had': True, 'bad': True, 'acting': True, 'and': True, 'terrible': True, 'photography': True}, 'neg')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI-ZLiIGHv_P",
        "outputId": "bb6073d8-4016-455e-e39c-1f9d8b42e80e"
      },
      "source": [
        "linearSVC = object_short_pickle_load[1]\n",
        "\n",
        "for i in range(0,4):\n",
        "  accuracy_linearSVC_classifier = nltk.classify.accuracy(linearSVC, short_features_set[i:i+1])\n",
        "  print(accuracy_linearSVC_classifier, i,  short_features_set[i:i+1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 0 [({'I': True, 'liked': True, 'it': True}, 'pos')]\n",
            "1.0 1 [({'It': True, 'was': True, 'bad': True}, 'neg')]\n",
            "1.0 2 [({'I': True, 'loved': True, 'it': True}, 'pos')]\n",
            "1.0 3 [({'It': True, 'had': True, 'bad': True, 'acting': True, 'and': True, 'terrible': True, 'photography': True}, 'neg')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0l5zkDmLpMZ"
      },
      "source": [
        "# Limpar o twit text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "KcSr9QSpLSfg",
        "outputId": "598611ec-c53e-48e4-cffb-01beea5c6768"
      },
      "source": [
        "print(myStreamListener.tweets[0][\"text\"])\n",
        "#separar o primeiro : do resto do twit\n",
        "\n",
        "filter_0 = re.search(r':', myStreamListener.tweets[0][\"text\"])\n",
        "#print(\"Filter_0\", filter_0)\n",
        "\n",
        "textFiltered_1 =myStreamListener.tweets[0][\"text\"][filter_0.span()[1]:]\n",
        "\n",
        "print(textFiltered_1)\n",
        "# Retirar os caracteres indesejados do twit\n",
        "\n",
        "\n",
        "filtered = re.search(':', myStreamListener.tweets[0][\"text\"])\n",
        "print(filtered)\n",
        "\n",
        "myStreamListener.tweets[0][\"text\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT @joshtpm: Entire AZ Legislature Shuts Down For A Week After Mask-Free Giuliani Contracts COVID-19 https://t.co/WdyW2d8W3m via @TPM\n",
            " Entire AZ Legislature Shuts Down For A Week After Mask-Free Giuliani Contracts COVID-19 https://t.co/WdyW2d8W3m via @TPM\n",
            "<_sre.SRE_Match object; span=(11, 12), match=':'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'RT @joshtpm: Entire AZ Legislature Shuts Down For A Week After Mask-Free Giuliani Contracts COVID-19 https://t.co/WdyW2d8W3m via @TPM'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPCqMlV-ftpV",
        "outputId": "94b5e4a7-c3b3-4bbc-efa2-b456e1450d67"
      },
      "source": [
        "print(myStreamListener.tweets[0][\"text\"][filtered.span()[1]:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 201207 MX Talk Tok\n",
            "\n",
            "[MONSTAX_KH]\n",
            "Monbebe, everyone have to wear mask well!!!!\n",
            "We really need to protect our bodies!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}